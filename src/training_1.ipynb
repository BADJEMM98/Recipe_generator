{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lTJ1KDVHpP34"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\badje\\OneDrive\\Bureau\\Cours_M2\\NLP\\projet\\Recipe_generator\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import GPT2Tokenizer, GPT2Config, TFGPT2Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BO25RJ6WpP3-"
      },
      "outputs": [],
      "source": [
        "current_dir = os.path.dirname(os.path.realpath(\"__file__\"))\n",
        "repo_dir = os.path.dirname(current_dir)\n",
        "DATA_FILE = os.path.join(repo_dir,\"data\",\"cleaned_data.csv\")\n",
        "data = pd.read_csv(DATA_FILE, encoding=\"utf-8\",sep=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Y6x1DUvZpP3-"
      },
      "outputs": [],
      "source": [
        "data.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5Cil65EpP3_",
        "outputId": "19c5a31e-21b0-4956-bd60-ffab1347553a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at gpt2 were not used when initializing TFGPT2Model: ['transformer/h_._8/ln_1/gamma:0', 'transformer/h_._6/ln_1/gamma:0', 'transformer/h_._8/mlp/c_proj/weight:0', 'transformer/h_._9/mlp/c_fc/bias:0', 'transformer/h_._8/mlp/c_proj/bias:0', 'transformer/h_._7/mlp/c_proj/bias:0', 'transformer/h_._6/attn/c_attn/bias:0', 'transformer/h_._7/ln_1/gamma:0', 'transformer/h_._8/ln_2/gamma:0', 'transformer/h_._10/ln_1/gamma:0', 'transformer/h_._9/attn/c_attn/bias:0', 'transformer/h_._9/ln_1/beta:0', 'transformer/h_._7/attn/c_attn/bias:0', 'transformer/h_._6/attn/c_proj/bias:0', 'transformer/h_._8/ln_1/beta:0', 'transformer/h_._11/mlp/c_proj/weight:0', 'transformer/h_._10/ln_2/gamma:0', 'transformer/h_._7/attn/c_proj/bias:0', 'transformer/h_._8/attn/c_attn/bias:0', 'transformer/h_._10/attn/c_proj/bias:0', 'transformer/h_._10/attn/c_proj/weight:0', 'transformer/h_._10/attn/c_attn/weight:0', 'transformer/h_._11/ln_2/beta:0', 'transformer/h_._11/attn/c_attn/weight:0', 'transformer/h_._8/mlp/c_fc/bias:0', 'transformer/h_._11/attn/c_proj/weight:0', 'transformer/h_._10/mlp/c_proj/bias:0', 'transformer/h_._6/attn/c_attn/weight:0', 'transformer/h_._9/attn/c_proj/weight:0', 'transformer/h_._9/mlp/c_proj/bias:0', 'transformer/h_._11/ln_1/beta:0', 'transformer/h_._6/mlp/c_proj/bias:0', 'transformer/h_._7/ln_1/beta:0', 'transformer/h_._8/mlp/c_fc/weight:0', 'transformer/h_._9/ln_1/gamma:0', 'transformer/h_._7/mlp/c_fc/bias:0', 'transformer/h_._9/mlp/c_proj/weight:0', 'transformer/h_._9/mlp/c_fc/weight:0', 'transformer/h_._6/ln_2/gamma:0', 'transformer/h_._6/ln_2/beta:0', 'transformer/h_._9/attn/c_attn/weight:0', 'transformer/h_._10/attn/c_attn/bias:0', 'transformer/h_._6/mlp/c_proj/weight:0', 'transformer/h_._11/mlp/c_fc/weight:0', 'transformer/h_._8/attn/c_attn/weight:0', 'transformer/h_._11/mlp/c_fc/bias:0', 'transformer/h_._11/ln_1/gamma:0', 'transformer/h_._8/attn/c_proj/bias:0', 'transformer/h_._7/attn/c_proj/weight:0', 'transformer/h_._6/ln_1/beta:0', 'transformer/h_._7/mlp/c_fc/weight:0', 'transformer/h_._11/attn/c_proj/bias:0', 'transformer/h_._10/mlp/c_proj/weight:0', 'transformer/h_._7/attn/c_attn/weight:0', 'transformer/h_._9/ln_2/beta:0', 'transformer/h_._7/mlp/c_proj/weight:0', 'transformer/h_._10/ln_1/beta:0', 'transformer/h_._10/mlp/c_fc/weight:0', 'transformer/h_._10/mlp/c_fc/bias:0', 'transformer/h_._7/ln_2/gamma:0', 'transformer/h_._7/ln_2/beta:0', 'transformer/h_._6/attn/c_proj/weight:0', 'transformer/h_._9/attn/c_proj/bias:0', 'transformer/h_._11/ln_2/gamma:0', 'transformer/h_._8/attn/c_proj/weight:0', 'transformer/h_._11/mlp/c_proj/bias:0', 'transformer/h_._6/mlp/c_fc/bias:0', 'transformer/h_._9/ln_2/gamma:0', 'transformer/h_._11/attn/c_attn/bias:0', 'transformer/h_._8/ln_2/beta:0', 'transformer/h_._10/ln_2/beta:0', 'transformer/h_._6/mlp/c_fc/weight:0']\n",
            "- This IS expected if you are initializing TFGPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFGPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFGPT2Model were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "config = GPT2Config.from_pretrained('gpt2',n_layer=6)\n",
        "config.output_hidden_states = False\n",
        "model = TFGPT2Model.from_pretrained('gpt2',config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JtpC6bEzpP3_"
      },
      "outputs": [],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', lang='fr')\n",
        "# tokenizer.pad_token = tokenizer.eos_token\n",
        "# model.resize_token_embeddings(len(tokenizer))\n",
        "# model.config.pad_token_id = model.config.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58KO5bKisgDd",
        "outputId": "285bf019-07d5-451c-8083-5271f49151d8"
      },
      "outputs": [],
      "source": [
        "input_sequences = []\n",
        "target_sequences = []\n",
        "for i in range(len(data)):\n",
        "    recipe = data.iloc[[i]].to_dict(\"records\")[0]\n",
        "    inputs = tokenizer.encode(recipe[\"Ingredients\"], truncation = True,return_tensors=\"tf\")\n",
        "    targets = tokenizer.encode(recipe[\"recettes\"], truncation = True,return_tensors=\"tf\")\n",
        "    input_sequences.append(inputs)\n",
        "    target_sequences.append(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "Fdo_lZmgD9l-",
        "outputId": "ea5b44ba-4506-4bb8-d21f-290a762300de"
      },
      "outputs": [],
      "source": [
        "max_len_input = max([input_sequences[i].shape[1] for i in range(len(input_sequences))])\n",
        "max_len_target = max([target_sequences[i].shape[1] for i in range(len(target_sequences))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "b1ATAzywCjCk"
      },
      "outputs": [],
      "source": [
        "max_length = max(max_len_input,max_len_target)\n",
        "\n",
        "# Pad the input and target sequences to the maximum length\n",
        "padded_input_sequences = []\n",
        "padded_target_sequences = []\n",
        "for inputs, targets in zip(input_sequences, target_sequences):\n",
        "    padded_inputs = tf.pad(inputs, [[0, max_length - inputs.shape[1]], [0, 0]])\n",
        "    padded_targets = tf.pad(targets, [[0, max_length - targets.shape[1]], [0, 0]])\n",
        "    padded_input_sequences.append(padded_inputs)\n",
        "    padded_target_sequences.append(padded_targets)\n",
        "\n",
        "input_sequences = padded_input_sequences\n",
        "target_sequences = padded_target_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ethCIP05zR5l",
        "outputId": "a0000e77-fe25-47c3-a9e1-0919e4ea990b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "957"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(padded_input_sequences[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "XoTi0QICu19i",
        "outputId": "eb94506a-e933-496e-87d5-892490337098"
      },
      "outputs": [
        {
          "ename": "MemoryError",
          "evalue": "Unable to allocate 13.8 GiB for an array with shape (46926, 1024, 77) and data type int32",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m input_sequences \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mpreprocessing\u001b[39m.\u001b[39;49msequence\u001b[39m.\u001b[39;49mpad_sequences(input_sequences, maxlen\u001b[39m=\u001b[39;49mmax_length, padding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m target_sequences \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mpreprocessing\u001b[39m.\u001b[39msequence\u001b[39m.\u001b[39mpad_sequences(target_sequences, maxlen \u001b[39m=\u001b[39m max_length, padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\badje\\OneDrive\\Bureau\\Cours_M2\\NLP\\projet\\Recipe_generator\\venv\\lib\\site-packages\\keras\\utils\\data_utils.py:1070\u001b[0m, in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m   1063\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m dtype \u001b[39m!=\u001b[39m \u001b[39mobject\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_dtype_str:\n\u001b[0;32m   1064\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1065\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`dtype` \u001b[39m\u001b[39m{\u001b[39;00mdtype\u001b[39m}\u001b[39;00m\u001b[39m is not compatible with `value`\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms type: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1066\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mYou should set `dtype=object` for variable length \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1067\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mstrings.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1068\u001b[0m     )\n\u001b[1;32m-> 1070\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mfull((num_samples, maxlen) \u001b[39m+\u001b[39;49m sample_shape, value, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m   1071\u001b[0m \u001b[39mfor\u001b[39;00m idx, s \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(sequences):\n\u001b[0;32m   1072\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(s):\n",
            "File \u001b[1;32mc:\\Users\\badje\\OneDrive\\Bureau\\Cours_M2\\NLP\\projet\\Recipe_generator\\venv\\lib\\site-packages\\numpy\\core\\numeric.py:344\u001b[0m, in \u001b[0;36mfull\u001b[1;34m(shape, fill_value, dtype, order, like)\u001b[0m\n\u001b[0;32m    342\u001b[0m     fill_value \u001b[39m=\u001b[39m asarray(fill_value)\n\u001b[0;32m    343\u001b[0m     dtype \u001b[39m=\u001b[39m fill_value\u001b[39m.\u001b[39mdtype\n\u001b[1;32m--> 344\u001b[0m a \u001b[39m=\u001b[39m empty(shape, dtype, order)\n\u001b[0;32m    345\u001b[0m multiarray\u001b[39m.\u001b[39mcopyto(a, fill_value, casting\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39munsafe\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    346\u001b[0m \u001b[39mreturn\u001b[39;00m a\n",
            "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 13.8 GiB for an array with shape (46926, 1024, 77) and data type int32"
          ]
        }
      ],
      "source": [
        "input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=max_length, padding=\"post\")\n",
        "target_sequences = tf.keras.preprocessing.sequence.pad_sequences(target_sequences, maxlen = max_length, padding=\"post\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHGRo3BLsFAy"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((input_sequences, target_sequences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayyfTyWspP4A",
        "outputId": "33b5a350-cc09-42e6-ab25-f935a26acd1b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2Model.\n",
            "\n",
            "All the layers of TFGPT2Model were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<transformers.modeling_tf_utils.TFSharedEmbeddings at 0x2449ef70e20>"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config = GPT2Config.from_pretrained('gpt2')\n",
        "config.output_hidden_states = False\n",
        "model = TFGPT2Model.from_pretrained('gpt2',config=config)\n",
        "# model.config = config\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2n1iE039pU3T"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(inputs, targets):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(inputs, labels=targets)[0]\n",
        "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=targets)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGDZ3g_Dwd16"
      },
      "outputs": [],
      "source": [
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, targets in dataset:\n",
        "        loss = train_step(inputs, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyXjOX0-pP4A"
      },
      "outputs": [],
      "source": [
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "X_test = np.expand_dims(X_test, axis=-1)\n",
        "y_train = np.expand_dims(y_train, axis=-1)\n",
        "y_test = np.expand_dims(y_test, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6J7nw0FPpP4A"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "\n",
        "X = sequences['input_ids'].numpy()\n",
        "y = sequences['attention_mask'].numpy()\n",
        "input_dim = X.shape[1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIbzoiDhpP4B",
        "outputId": "d40e0278-c1de-48a4-da89-259c6ef90266"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        }
      ],
      "source": [
        "# Compile the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
        "\n",
        "model.compile(\n",
        "  loss=loss,\n",
        "  optimizer=optimizer,\n",
        "  metrics=metric\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "  X_train,\n",
        "  y_train,\n",
        "  batch_size=128,\n",
        "  epochs=10,\n",
        "  validation_data=(X_test, y_test)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "6XuvWbDMpP3_",
        "outputId": "9aaccd66-d504-44fe-c847-025913ad0ff8"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f7a8d195fcbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'recettes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# num_classes = len(np.unique(y))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# y = tf.keras.utils.to_categorical(y, num_classes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2795\u001b[0m         )\n\u001b[1;32m   2796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2797\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   2798\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids_or_pair_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m             \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m             \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    699\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mis_split_into_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mconvert_tokens_to_ids\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0mids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_token_to_id_with_added_voc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_convert_token_to_id_with_added_voc\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madded_tokens_encoder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madded_tokens_encoder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_token_to_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_convert_token_to_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/gpt2/tokenization_gpt2.py\u001b[0m in \u001b[0;36m_convert_token_to_id\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_convert_token_to_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;34m\"\"\"Converts a token (str) in an id using the vocab.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munk_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_convert_id_to_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "max_length = 512\n",
        "input_data = \n",
        "sequences = tokenizer.batch_encode_plus(data['recettes'].to_list(), padding=True, truncation=True, return_tensors='tf', max_length=max_length)\n",
        "\n",
        "# num_classes = len(np.unique(y))\n",
        "# y = tf.keras.utils.to_categorical(y, num_classes)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "befec533deea68de54ca62dbffdf5cda37e71d4b7ad2a5c99b38a8c68b455b77"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
